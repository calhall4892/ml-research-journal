{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Mathematics of Linear Regression\n",
        "\n",
        "_Author: Callum Hall_  \n",
        "_Date: 05/08/2025_"
      ],
      "metadata": {
        "id": "cqyeV5R7QNg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1: The Core Idea\n",
        "\n",
        "## 1.1 Overview\n",
        "\n",
        "Linear Regression is a type of supervised algorithm that uses labelled data, consisting of input features ($X_{train}$) and their corresponding true values ($y_{train}$). The goal is to learn the most optimised linear function that can then be applied and used to predict the value ($\\hat{y}$) on a new, unseen dataset ($X_{test}$).\n",
        "\n",
        "## 1.2 The General Equation\n",
        "\n",
        "As previously desribed in the overview, the algorithm uses a linear function to achieve the best optimised result. This, therefore, means the algorithm is ultimatly based upon $y = mx + c$ but this is displayed differently in machine learning and is as follows $$\\hat{y} = Wx + b$$\n",
        "\n",
        "* $\\hat{y}$ - Dependant variable (prediction). $y$ is used to to represent true value.\n",
        "* $W$ - Gradient (weights)\n",
        "* $x$ - Independant variable ($X$ for matrix)\n",
        "* $b$ - Y-Interecept (bias)"
      ],
      "metadata": {
        "id": "3tzZcf-DYEDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2: The Loss Function\n",
        "\n",
        "## 2.1 The General Formula\n",
        "\n",
        "Mean Squared Error (MSE) is a fundamental metric used within ML, particulally within regression models. It works by evaluating the difference between the predicted values and the actual values and gives a squared value.\n",
        "\n",
        "$$MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "* $n$ - The number of datapoints\n",
        "* $y_i$ - The actual data\n",
        "* $ \\hat{y}_i$ - The predicted data\n",
        "\n",
        "The purpose of squaring the results is for number of reasons. First is to give greater weight to larger errors. If we have an error value of 2 then $2^2 = 4$ whereas and error of 10 is $10^2 = 100$.\n",
        "\n",
        "A further purpose is to square all errors to ensure that model evaluation is accurate. This can be seen here\n",
        "\n",
        "$$£105 - £100 = + £5$$\n",
        "$$£95 - £100 = - £5$$\n",
        "\n",
        "Here if we add up the error values we get 0, therefore giving an incorrect sense that the model is perfect. Therefore we square.\n",
        "\n",
        "$$(+£5)^2 = 25$$\n",
        "$$(-£5)^2 = 25$$\n",
        "\n",
        "It should be noted that this is a loss function and not an evaluation metric. This is down to the difficulty in humans making sense of the results as the results come out in $(unit)^2$, meaning it could be £ squared etc. A better version could be RMSE which ensures this is not a problem. This is discussed in this notebook."
      ],
      "metadata": {
        "id": "s2WYqyyC_h1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Goal of Optimisation\n",
        "\n",
        "## 3.1 Gradient Decent\n",
        "\n",
        "The goal of optimisation is to reduce the loss until 0. This is not realistic in practice so therefore this is done until there is no noticeable reduction in loss. This often leads to the introduction of early stopping procedures within the code. This works by stopping the process once there is no longer $n$ change of loss within $x$ epochs.\n",
        "\n",
        "Gradient decent is an optimisation algorithm that aims to make the loss as small as possible. To do this, it find the derivative of the slope and uses the sign, (+ or -) to work out which was it needs to adjust to decrease the $W$ and $b$. The magnitude of the derivative instructs on how big of a step should be taken. (Covered in more detail in the Fundamentals)\n",
        "\n",
        "### 3.11 The General Formula\n",
        "\n",
        "$$N_{parameter} = O_{parameter} - \\alpha * gradient$$\n",
        "\n",
        "For $W$ and $b$ parameters, the update rules are\n",
        "\n",
        "$$W_{new} = W_{old} - \\alpha * \\frac{\\partial L}{\\partial W}$$\n",
        "\n",
        "$$b_{new} = b_{old} - \\alpha * \\frac{\\partial L}{\\partial b}$$\n",
        "\n",
        "The formulas are applied in the following method\n",
        "\n",
        "* $N$ - New\n",
        "* $O$ - Old\n",
        "* $\\alpha$ - Learning Rate\n",
        "* $L$ - Loss Function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8guDO6BTRn6K"
      }
    }
  ]
}